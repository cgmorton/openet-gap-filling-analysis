{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0566f07-3636-49a6-ac89-2e49c389a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import ee\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "STORAGE_CLIENT = storage.Client(project='openet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a776b2-33ef-45fa-ac01-fb18ca7dbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize(project='ee-cmorton', opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059ed2cf-c722-456c-abeb-ba6fc1fed00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum number of months in the target year\n",
    "min_month_count = 3\n",
    "\n",
    "# TODO: Add support for setting a minimum number of months in the year\n",
    "#   and minimum number of months in the growing season\n",
    "# min_month_count = 6\n",
    "# min_gs_month_count = 3\n",
    "\n",
    "# Exclude 2016 from the statistics since there is not a full prior year to interpolate from\n",
    "# Including 2024 even though 2025 is not complete\n",
    "stats_years = list(range(2017, 2025))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b344f6d7-85d4-43ce-b324-5507324660a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGM - We could pull separate sets of points for each NLCD year but this doesn't seem that useful\n",
    "nlcd_years = [2024]\n",
    "# nlcd_years = [2024, 2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016]\n",
    "\n",
    "ensemble_coll_id = 'projects/openet/assets/ensemble/conus/gridmet/monthly/v2_1'\n",
    "reference_et_coll_id = 'projects/openet/assets/reference_et/conus/gridmet/monthly/v1'\n",
    "nlcd_coll = (\n",
    "    ee.ImageCollection('projects/sat-io/open-datasets/USGS/ANNUAL_NLCD/LANDCOVER')\n",
    "    .select([0], [\"landcover\"])\n",
    ")\n",
    "mgrs_coll = (\n",
    "    ee.FeatureCollection('projects/openet/assets/mgrs/tiles/modified')\n",
    "    .filterBounds(ee.Geometry.BBox(-125, 25.5, -66.5, 48.5))\n",
    "    .filterBounds(\n",
    "        ee.Image(\"projects/openet/assets/features/water_mask_buffer\")\n",
    "        .eq(0).rename(['mask']).geometry()\n",
    "    )\n",
    ")\n",
    "mgrs_zones = [\n",
    "    '10S', '10T', '10U', '11S', '11T', '11U', '12S', '12T', '12U', \n",
    "    '13R', '13S', '13T', '13U', '14R', '14S', '14T', '14U', '15R', '15S', '15T', '15U', \n",
    "    '16R', '16S', '16T', '17R', '17S', '17T', '18S', '18T', '19T'\n",
    "    # These two zones are too small and don't have enough points\n",
    "    # '12R', '16U'\n",
    "]\n",
    "\n",
    "# Identify the MGRS tiles that have a centroid that intersects the GRIDMET mask\n",
    "def ftr_area(ftr):\n",
    "    return ftr.set({\n",
    "        'mask': ee.Image('projects/openet/assets/mgrs/conus/gridmet/data_mask')\n",
    "            .unmask().reduceRegion(ee.Reducer.first(), ftr.geometry().centroid(), 4000).get('mask'),\n",
    "        # 'area': ftr.geometry().area(),\n",
    "    })\n",
    "mgrs_coll = ee.FeatureCollection(mgrs_coll.map(ftr_area)).filterMetadata('mask', 'equals', 1)\n",
    "mgrs_tiles = sorted(list(mgrs_coll.aggregate_array('mgrs').getInfo()))\n",
    "mgrs_tile_skip_list = ['15UUQ', '16RBT', '17RNM', '18SVE', '19TEN']\n",
    "\n",
    "# Local folder for points CSV files\n",
    "points_folder = 'points'\n",
    "if not os.path.isdir(points_folder):\n",
    "    os.makedirs(points_folder)\n",
    "\n",
    "points_csv = 'gap_fill_test_points.csv'\n",
    "points_coll_id = 'projects/ee-cmorton/assets/gap_fill_test_points'\n",
    "\n",
    "bucket_name = 'openet_temp'\n",
    "bucket = STORAGE_CLIENT.bucket(bucket_name)\n",
    "\n",
    "months = list(range(1, 13))\n",
    "\n",
    "print_n = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab033a0-b3f7-4f10-94b2-796a2521df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLCD: 2024\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Build a list of test points stratified by NLCD type\n",
    "# Using a default of 2 points so that the edge tiles can drop to having 1 point\n",
    "max_points = 2\n",
    "overwrite_flag = False\n",
    "\n",
    "# Building separate lists by year with the idea being that the NLCD can be different per year\n",
    "# It would probably make more sense to build a single point list \n",
    "#   and then extract the NLCD values for each/all years\n",
    "# for nlcd_year in nlcd_years:\n",
    "for nlcd_year in [2024]:\n",
    "    print(f'NLCD: {nlcd_year}')\n",
    "\n",
    "    nlcd_img = nlcd_coll.filterDate(f'{nlcd_year}-01-01', f'{nlcd_year+1}-01-01').first()\n",
    "\n",
    "    # Masking out water, perennial ice/snow, and urban land covers\n",
    "    nlcd_img = nlcd_img.updateMask(\n",
    "        nlcd_img.eq(11).Or(nlcd_img.eq(12))\n",
    "        .Or(nlcd_img.eq(21)).Or(nlcd_img.eq(22)).Or(nlcd_img.eq(23))\n",
    "        .Not()\n",
    "    )   \n",
    "     \n",
    "    for mgrs_zone in mgrs_zones:\n",
    "        points_mgrs_year_csv = os.path.join(points_folder, f'points_{mgrs_zone}_{nlcd_year}.csv')\n",
    "        if os.path.isfile(points_mgrs_year_csv) and not overwrite_flag:\n",
    "            # print(f'{mgrs_zone} - csv already exists, skipping')\n",
    "            continue\n",
    "        print(f'{mgrs_zone}')        \n",
    "\n",
    "        point_list = []\n",
    "        for mgrs_tile in mgrs_tiles:\n",
    "            if mgrs_tile[:3] != mgrs_zone:\n",
    "                continue\n",
    "            elif mgrs_tile in mgrs_tile_skip_list:\n",
    "                continue\n",
    "            \n",
    "            mgrs_geom = mgrs_coll.filterMetadata('mgrs', 'equals', mgrs_tile).first().geometry()\n",
    "            \n",
    "            # Adjust the minimum number of points based on the size of the MGRS tile\n",
    "            mgrs_points = mgrs_geom.area().divide(10000000000).multiply(max_points).round().clamp(0, max_points).getInfo()\n",
    "            print(f'  {mgrs_tile} - Points: {mgrs_points}')\n",
    "            if mgrs_points == 0:\n",
    "                continue\n",
    "        \n",
    "            point_coll = nlcd_img.stratifiedSample(\n",
    "                numPoints=mgrs_points, \n",
    "                classBand='landcover', \n",
    "                region=mgrs_geom, \n",
    "                dropNulls=True, \n",
    "                geometries=True,\n",
    "                seed=nlcd_year, \n",
    "            )\n",
    "            try:\n",
    "                point_info = point_coll.getInfo()['features']\n",
    "            except Exception as e:\n",
    "                print('  Error getting point collection, skipping')\n",
    "                continue\n",
    "            \n",
    "            point_list.extend([\n",
    "                {\n",
    "                    'latitude': pnt['geometry']['coordinates'][1],\n",
    "                    'longitude': pnt['geometry']['coordinates'][0],\n",
    "                    'mgrs_tile': mgrs_tile, \n",
    "                    'mgrs_zone': mgrs_zone, \n",
    "                    'nlcd': pnt['properties']['landcover'], \n",
    "                    'nlcd_year': nlcd_year, \n",
    "                }\n",
    "                for pnt in point_info\n",
    "            ])\n",
    "\n",
    "        print(f'  Total Points: {len(point_list)}')\n",
    "        pd.DataFrame(point_list).to_csv(points_mgrs_year_csv, index=False)\n",
    "\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110c7f91-bb58-46f5-9e08-43ea30237d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points: 17084\n"
     ]
    }
   ],
   "source": [
    "# Building a single points dataframe and CSV from the MGRS grid zone points CSV files\n",
    "overwrite_flag = False\n",
    "\n",
    "# Read the separate points CSV files into a single dataframe\n",
    "points_df_list = [\n",
    "    pd.read_csv(os.path.join(points_folder, f'points_{mgrs_zone}_{nlcd_year}.csv'), index_col=None, header=0)\n",
    "    for nlcd_year in nlcd_years\n",
    "    for mgrs_zone in mgrs_zones\n",
    "    if os.path.isfile(os.path.join(points_folder, f'points_{mgrs_zone}_{nlcd_year}.csv'))\n",
    "]\n",
    "points_df = pd.concat(points_df_list, axis=0, ignore_index=True)\n",
    "print(f'Points: {len(points_df.index)}')\n",
    "\n",
    "# The mgrs_zone value will eventually be added to the csv files\n",
    "points_df['mgrs_zone'] = points_df['mgrs_tile'].str.slice(0, 3)\n",
    "\n",
    "# Add a unique index to the points dataframe\n",
    "points_df['index_group'] = points_df.groupby(['mgrs_tile', 'nlcd']).cumcount()\n",
    "points_df['point_id'] = (\n",
    "    points_df[\"mgrs_tile\"].str.upper() + '_' +\n",
    "    'nlcd' + points_df[\"nlcd\"].astype(str).str.zfill(2) + '_' +\n",
    "    points_df[\"index_group\"].astype(str).str.zfill(2)\n",
    ")\n",
    "del points_df['index_group']\n",
    "\n",
    "# Round the lat and lon to 8 decimal places (probably should be 6)\n",
    "points_df['latitude'] = round(points_df['latitude'], 8)\n",
    "points_df['longitude'] = round(points_df['longitude'], 8)\n",
    "\n",
    "# Write to CSV\n",
    "# print(points_df.head())\n",
    "if not os.path.isfile(points_csv) or overwrite_flag:\n",
    "    print('Writing points csv')\n",
    "    points_df.to_csv(points_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d862ae8-3b79-4cea-ac91-0dcfe809a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building points feature collection\n",
    "overwrite_flag = False\n",
    "\n",
    "# Upload to bucket\n",
    "bucket_path = f'gs://{bucket_name}/{points_csv}'\n",
    "blob = bucket.blob(bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "if overwrite_flag or not blob.exists():\n",
    "    print('Uploading csv to bucket')\n",
    "    blob = bucket.blob(bucket_path.replace(f'gs://{bucket_name}/', ''))\n",
    "    blob.upload_from_filename(points_csv)\n",
    "\n",
    "# Ingest into GEE\n",
    "if overwrite_flag and ee.data.getInfo(points_coll_id):\n",
    "    print('Removing existing collection')\n",
    "    ee.data.deleteAsset(points_coll_id)\n",
    "    \n",
    "if not ee.data.getInfo(points_coll_id):\n",
    "    print('Starting csv ingest')\n",
    "    manifest = {'name': points_coll_id, 'sources': [{'uris': [bucket_path]}]}\n",
    "    ee.data.startTableIngestion(None, manifest)\n",
    "\n",
    "# DEADBEEF - The full collection is to large to export directly to asset\n",
    "# # Build the feature collection and export to asset\n",
    "# points_coll = ee.FeatureCollection([\n",
    "#     ee.Feature(\n",
    "#         ee.Geometry.Point(round(point['lon'], 6), round(point['lat'], 6)), \n",
    "#         {\n",
    "#             'point_id': point['point_id'],\n",
    "#             'mgrs': point['mgrs'], \n",
    "#             'nlcd': point['nlcd'], \n",
    "#             'nlcd_year': point['nlcd_year'], \n",
    "#         }\n",
    "#     )\n",
    "#     for point_i, point in point_df.iterrows()\n",
    "# ])\n",
    "\n",
    "# if not ee.data.getInfo(points_coll_id):\n",
    "#     task = ee.batch.Export.table.toAsset(\n",
    "#         collection=points_coll, \n",
    "#         description='openet_gap_filling_test_points', \n",
    "#         assetId=points_coll_id, \n",
    "#     )\n",
    "#     task.start()\n",
    "#     task_id = task.status()['id']\n",
    "#     print(f'{task_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3f5d69-869f-40d3-bc28-bac896b9cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Pull ensemble ET and ETo time series at all points\n",
    "# Run extractions by MGRS tile to better control crs and transform parameters?\n",
    "\n",
    "overwrite_flag = False\n",
    "\n",
    "for mgrs_zone in mgrs_zones:\n",
    "\n",
    "    data_mgrs_csv = os.path.join('data', f'data_{mgrs_zone}.csv')\n",
    "    if os.path.isfile(data_mgrs_csv) and not overwrite_flag:\n",
    "        # print(f'{mgrs_zone} - csv file already exists, skipping')\n",
    "        continue\n",
    "        \n",
    "    mgrs_points_coll = (\n",
    "        ee.FeatureCollection(points_coll_id)\n",
    "        .filterMetadata('mgrs_zone', 'equals', mgrs_zone)\n",
    "        # .filterMetadata('nlcd_year', 'equals', 2024)\n",
    "    )\n",
    "    print(f'{mgrs_zone} - Points: {mgrs_points_coll.size().getInfo()}')\n",
    "    # pprint.pprint(mgrs_points_coll.getInfo())\n",
    "    \n",
    "    # TODO: Lookup all the crs and transform values for the MGRS tile list ahead of time\n",
    "    # Get the crs and transform for the MGRS tile from one of the ensemble images\n",
    "    mgrs_info = (\n",
    "        ee.ImageCollection(ensemble_coll_id)\n",
    "        .filterMetadata('mgrs_tile', 'equals', mgrs_zone)\n",
    "        .first().select(['et_ensemble_mad']).getInfo()\n",
    "    )\n",
    "\n",
    "    # Get the list of images to read from \n",
    "    image_id_list = (\n",
    "        ee.ImageCollection(ensemble_coll_id)\n",
    "        .filterDate('2015-10-01', '2025-10-01')\n",
    "        .filterMetadata('mgrs_tile', 'equals', mgrs_zone)\n",
    "        .aggregate_array('system:index').getInfo()\n",
    "    )\n",
    "    \n",
    "    # Extract values\n",
    "    image_df_list = []\n",
    "    for image_i, image_id in enumerate(image_id_list):\n",
    "        if image_i % 10 == 0:\n",
    "            print(image_i, image_id)\n",
    "\n",
    "        src_img = ee.Image(f'{ensemble_coll_id}/{image_id}')\n",
    "        image_date = ee.Date(src_img.get('system:time_start'))\n",
    "\n",
    "        # I was getting weird errors when trying to get the ETo and ensemble in one call\n",
    "        #   so pulling ETo separately using scale parameter\n",
    "        eto_img = (\n",
    "            ee.ImageCollection(reference_et_coll_id)\n",
    "            .filterDate(image_date.advance(-1, 'day'), image_date.advance(1, 'day')).first()\n",
    "            .select(['eto'])\n",
    "            .resample('bilinear')\n",
    "        )\n",
    "        eto_values_coll = eto_img.reduceRegions(\n",
    "            collection=mgrs_points_coll, \n",
    "            reducer=ee.Reducer.first(), \n",
    "            scale=30,\n",
    "            #crs=mgrs_info['bands'][0]['crs'], \n",
    "            #crsTransform=mgrs_info['bands'][0]['crs_transform'], \n",
    "        )\n",
    "        try:\n",
    "            eto_values = eto_values_coll.getInfo()\n",
    "        except Exception as e:\n",
    "            print(f'ETo Exception: {e}')\n",
    "            continue\n",
    "        eto_values = {\n",
    "            ftr['properties']['point_id']: round(ftr['properties']['first']) \n",
    "            for ftr in eto_values['features']\n",
    "            if 'first' in ftr['properties'].keys()\n",
    "        }\n",
    "\n",
    "        # Extract the ensemble ET and count\n",
    "        image_values_coll = (\n",
    "            src_img.select(['et_ensemble_mad', 'et_ensemble_mad_count'], ['et', 'count'])\n",
    "            .reduceRegions(\n",
    "                collection=mgrs_points_coll, \n",
    "                reducer=ee.Reducer.first(), \n",
    "                crs=mgrs_info['bands'][0]['crs'], \n",
    "                crsTransform=mgrs_info['bands'][0]['crs_transform'], \n",
    "            )\n",
    "        )\n",
    "        try:\n",
    "            image_values = image_values_coll.getInfo()\n",
    "        except Exception as e:\n",
    "            print(f'  ET Exception: {e}')\n",
    "            print(f'  {image_id}')\n",
    "            continue\n",
    "        \n",
    "        # Build a dataframe of all the daily values\n",
    "        # Use the ETo value to determine if the point is valid and should be kept\n",
    "        image_df = [\n",
    "            {\n",
    "                'date': datetime.strptime(image_id.split('_')[1], '%Y%m%d').strftime('%Y-%m-%d'), \n",
    "                'point_id': pnt['properties']['point_id'], \n",
    "                'mgrs_tile': pnt['properties']['mgrs_tile'], \n",
    "                'mgrs_zone': pnt['properties']['mgrs_zone'], \n",
    "                'nlcd': pnt['properties']['nlcd'], \n",
    "                'nlcd_year': pnt['properties']['nlcd_year'], \n",
    "                'count': pnt['properties']['count'],\n",
    "                'et': pnt['properties']['et'], \n",
    "                'eto': eto_values[pnt['properties']['point_id']], \n",
    "            } \n",
    "            for pnt in image_values['features']\n",
    "            if pnt['properties']['point_id'] in eto_values.keys() and eto_values[pnt['properties']['point_id']]\n",
    "        ]\n",
    "        image_df_list.extend(image_df)\n",
    "\n",
    "    pd.DataFrame(image_df_list).to_csv(data_mgrs_csv, index=False)\n",
    "\n",
    "    del mgrs_points_coll, mgrs_info, image_id_list, image_df_list\n",
    "\n",
    "print('\\nDone')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
